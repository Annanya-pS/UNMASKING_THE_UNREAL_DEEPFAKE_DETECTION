{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9146200,"sourceType":"datasetVersion","datasetId":5524489},{"sourceId":13861656,"sourceType":"datasetVersion","datasetId":8830302}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi\nimport tensorflow as tf\nprint(\"GPUs detected:\", tf.config.list_physical_devices('GPU'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rm -rf /kaggle/working/*\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Pre-processing + Model training**","metadata":{}},{"cell_type":"code","source":"# ========== Install Required Packages with Correct Protobuf ==============\n!pip install protobuf==3.20.3 mediapipe tensorflow opencv-python-headless scikit-learn matplotlib seaborn albumentations -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:10:25.580394Z","iopub.execute_input":"2025-11-25T05:10:25.580600Z","iopub.status.idle":"2025-11-25T05:11:47.276949Z","shell.execute_reply.started":"2025-11-25T05:10:25.580579Z","shell.execute_reply":"2025-11-25T05:11:47.276041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# PROTOBUF FIX \n# ======================================================================\nimport sys\nimport subprocess\n\ndef fix_protobuf():\n    \"\"\"Fix protobuf compatibility issue with MediaPipe\"\"\"\n    try:\n        import google.protobuf\n        protobuf_version = google.protobuf.__version__\n        print(f\"Current protobuf version: {protobuf_version}\")\n        \n        if protobuf_version.startswith('4.') or protobuf_version.startswith('5.'):\n            print(\"⚠️  Incompatible protobuf version detected. Downgrading...\")\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n                                 \"protobuf==3.20.3\", \"-q\"])\n            print(\"✓ Protobuf downgraded to 3.20.3\")\n            print(\"⚠️  Please restart the kernel and run again!\")\n            return False\n        else:\n            print(f\"✓ Protobuf version {protobuf_version} is compatible\")\n            return True\n    except Exception as e:\n        print(f\"Error checking protobuf: {e}\")\n        return True\n\nif not fix_protobuf():\n    print(\"\\n\" + \"=\"*80)\n    print(\"KERNEL RESTART REQUIRED\")\n    print(\"=\"*80)\n    sys.exit(0)\n\n# ======================================================================\n# IMPORT ALL LIBRARIES INCLUDING TF BEFORE USING tf.config\n# ======================================================================\nimport os\nimport glob\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport gc\nimport json\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\n# Only AFTER TensorFlow is imported:\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, Callback\nfrom tensorflow.keras import regularizers\n\nimport mediapipe as mp\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.utils.class_weight import compute_class_weight\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(\"✓ All libraries imported successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:11:47.279628Z","iopub.execute_input":"2025-11-25T05:11:47.279968Z","iopub.status.idle":"2025-11-25T05:12:08.199384Z","shell.execute_reply.started":"2025-11-25T05:11:47.279942Z","shell.execute_reply":"2025-11-25T05:12:08.198169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef configure_cpu():\n    \"\"\"Configure TensorFlow for CPU-only operation\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"CPU CONFIGURATION\")\n    print(\"=\"*80)\n    \n    # Disable GPU completely\n    tf.config.set_visible_devices([], 'GPU')\n    \n    # Set CPU threading for stability\n    tf.config.threading.set_intra_op_parallelism_threads(2)\n    tf.config.threading.set_inter_op_parallelism_threads(2)\n    \n    # Disable XLA and mixed precision\n    tf.config.optimizer.set_jit(False)\n    \n    print(\"✓ CPU-only mode enabled\")\n    print(\"✓ Threading: 2 intra-op, 2 inter-op\")\n    print(\"✓ XLA disabled for stability\")\n    \n    return True\n\nconfigure_cpu()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:12:08.200859Z","iopub.execute_input":"2025-11-25T05:12:08.201511Z","iopub.status.idle":"2025-11-25T05:12:08.213033Z","shell.execute_reply.started":"2025-11-25T05:12:08.201490Z","shell.execute_reply":"2025-11-25T05:12:08.212016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import platform\nimport tensorflow as tf\nimport numpy as np\n\nprint(\"Python version:\", platform.python_version())\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"NumPy version:\", np.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:12:08.213908Z","iopub.execute_input":"2025-11-25T05:12:08.214178Z","iopub.status.idle":"2025-11-25T05:12:08.234518Z","shell.execute_reply.started":"2025-11-25T05:12:08.214157Z","shell.execute_reply":"2025-11-25T05:12:08.233796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MinimalConfig:\n    \"\"\"Ultra-minimal configuration for CPU training\"\"\"\n    \n    # Paths\n    BASE_INPUT_PATH = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset\"\n    BASE_OUTPUT_PATH = '/kaggle/working'\n    \n    # Preprocessing \n    FRAMES_PER_VIDEO = 10  \n    TARGET_SIZE = (224, 224)\n    FACE_PADDING = 0.2\n    MIN_FACE_SIZE = 100\n    FACE_CONFIDENCE = 0.5\n    JPEG_QUALITY = 90\n    \n    # Training \n    BATCH_SIZE = 8 \n    EPOCHS = 15\n    LEARNING_RATE = 1e-4\n    EARLY_STOP_PATIENCE = 4\n    REDUCE_LR_PATIENCE = 2\n    VALIDATION_SPLIT = 0.2\n    TEST_SPLIT = 0.2\n    \n    # Model\n    BACKBONE = 'EfficientNetB0'\n    UNFREEZE_LAYERS = 10\n    DROPOUT_RATE = 0.3\n    L2_REGULARIZATION = 0.0001\n    DENSE_UNITS = 128\n    \n    # Memory management\n    MAX_VIDEOS_TO_PROCESS = None \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# SIMPLE PREDICTION MONITOR\n# ============================================================================\n\nclass SimplePredictionMonitor(Callback):\n    \n    def __init__(self, val_files, val_labels, max_samples=50):\n        super().__init__()\n        self.val_files = val_files[:max_samples]\n        self.val_labels = val_labels[:max_samples]\n        \n    def on_epoch_end(self, epoch, logs=None):\n        predictions = []\n        \n        for i, filepath in enumerate(self.val_files):\n            try:\n                img = cv2.imread(filepath)\n                if img is None:\n                    continue\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                img = img.astype(np.float32)\n                img = np.expand_dims(img, axis=0)\n                img = efficientnet_preprocess(img)\n                \n                pred = self.model.predict(img, verbose=0)[0][0]\n                predictions.append(pred)\n            except:\n                continue\n            \n            if i >= 49:  # Only check 50 samples max\n                break\n        \n        if predictions:\n            pred_array = np.array(predictions)\n            print(f\"  [Pred] mean={pred_array.mean():.4f}, std={pred_array.std():.4f}\")\n            \n            if pred_array.std() < 0.05:\n                print(f\"WARNING: Prediction collapse detected\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MINIMAL FACE EXTRACTOR\n# ============================================================================\n\nclass MinimalFaceExtractor:\n    \n    def __init__(self, config=MinimalConfig):\n        self.config = config\n        self.mp_face = mp.solutions.face_detection\n        self.detector = self.mp_face.FaceDetection(\n            model_selection=1,\n            min_detection_confidence=config.FACE_CONFIDENCE\n        )\n        self.stats = defaultdict(int)\n    \n    def extract_faces_minimal(self, video_path):\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            return None\n        \n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        if total_frames < self.config.FRAMES_PER_VIDEO:\n            cap.release()\n            return None\n        \n        # Calculate frame indices to extract\n        frame_indices = np.linspace(0, total_frames - 1, self.config.FRAMES_PER_VIDEO, dtype=int)\n        \n        faces = []\n        \n        for target_idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, target_idx)\n            ret, frame = cap.read()\n            \n            if not ret or frame is None:\n                continue\n            \n            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            results = self.detector.process(rgb)\n            \n            if results.detections:\n                detection = results.detections[0]\n                bbox = detection.location_data.relative_bounding_box\n                \n                h, w = frame.shape[:2]\n                padding = self.config.FACE_PADDING\n                \n                x = max(0, int((bbox.xmin - padding * bbox.width) * w))\n                y = max(0, int((bbox.ymin - padding * bbox.height) * h))\n                bw = int(bbox.width * (1 + 2 * padding) * w)\n                bh = int(bbox.height * (1 + 2 * padding) * h)\n                \n                x2 = min(w, x + bw)\n                y2 = min(h, y + bh)\n                \n                if (x2 - x) >= self.config.MIN_FACE_SIZE and (y2 - y) >= self.config.MIN_FACE_SIZE:\n                    face = frame[y:y2, x:x2]\n                    \n                    if face.size > 0:\n                        face_resized = cv2.resize(\n                            face, \n                            self.config.TARGET_SIZE, \n                            interpolation=cv2.INTER_LINEAR\n                        )\n                        faces.append(face_resized)\n        \n        cap.release()\n        \n        del rgb, frame\n        gc.collect()\n        \n        return faces if len(faces) == self.config.FRAMES_PER_VIDEO else None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" def process_video(self, video_path, output_dir, label, video_id):\n        \"\"\"Process video and save exactly FRAMES_PER_VIDEO face crops\"\"\"\n        faces = self.extract_faces_minimal(video_path)\n        \n        if faces is None:\n            self.stats['videos_no_faces'] += 1\n            return 0\n        \n        self.stats['videos_processed'] += 1\n        saved_count = 0\n        \n        for idx, face_img in enumerate(faces):\n            filename = f\"{label}_{video_id:08d}_{idx:04d}_face.jpg\"\n            face_path = os.path.join(output_dir, filename)\n            \n            success = cv2.imwrite(\n                face_path, \n                face_img,\n                [cv2.IMWRITE_JPEG_QUALITY, self.config.JPEG_QUALITY]\n            )\n            \n            if success:\n                saved_count += 1\n                self.stats['faces_saved'] += 1\n        \n        del faces\n        gc.collect()\n        \n        return saved_count\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# MINIMAL DATA GENERATOR\n# ============================================================================\n\nclass UltraMinimalGenerator(tf.keras.utils.Sequence):\n    \"\"\"One-image-at-a-time generator - no batch preloading\"\"\"\n    \n    def __init__(self, file_paths, labels, batch_size=8, shuffle=True, augment=False):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.indices = np.arange(len(file_paths))\n        self.on_epoch_end()\n    \n    def __len__(self):\n        return int(np.ceil(len(self.file_paths) / self.batch_size))\n    \n    def __getitem__(self, index):\n        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n        \n        X_batch = []\n        y_batch = []\n        \n        for idx in batch_indices:\n            face_path = self.file_paths[idx]\n            label = self.labels[idx]\n            \n            # Load ONE image at a time\n            face_img = cv2.imread(face_path)\n            \n            if face_img is None:\n                continue\n            \n            face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n            \n            # Only horizontal flip augmentation\n            if self.augment and np.random.rand() > 0.5:\n                face_img = cv2.flip(face_img, 1)\n            \n            face_img = face_img.astype(np.float32)\n            \n            X_batch.append(face_img)\n            y_batch.append(label)\n        \n        if len(y_batch) == 0:\n            dummy_img = np.zeros((224, 224, 3), dtype=np.float32)\n            return np.array([dummy_img]), np.array([0], dtype=np.float32)\n        \n        X_batch = np.array(X_batch, dtype=np.float32)\n        y_batch = np.array(y_batch, dtype=np.float32)\n        \n        # Preprocess\n        X_batch = efficientnet_preprocess(X_batch)\n        \n        return X_batch, y_batch\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# LIGHTWEIGHT MODEL BUILDER\n# ============================================================================\n\ndef build_minimal_model(config=MinimalConfig):\n    \"\"\"Build EfficientNetB0 model for CPU\"\"\"\n    \n    input_layer = Input(shape=(*config.TARGET_SIZE, 3), name='face_input')\n    \n    base = EfficientNetB0(\n        weights='imagenet', \n        include_top=False, \n        input_tensor=input_layer\n    )\n    \n    # Freeze most layers\n    total_layers = len(base.layers)\n    freeze_until = total_layers - config.UNFREEZE_LAYERS\n    \n    for i, layer in enumerate(base.layers):\n        layer.trainable = (i >= freeze_until)\n    \n    x = GlobalAveragePooling2D(name='gap')(base.output)\n    x = BatchNormalization(name='bn1')(x)\n    x = Dropout(config.DROPOUT_RATE, name='dropout1')(x)\n    \n    x = Dense(\n        config.DENSE_UNITS, \n        activation='relu',\n        kernel_regularizer=regularizers.l2(config.L2_REGULARIZATION),\n        name='fc1'\n    )(x)\n    x = BatchNormalization(name='bn2')(x)\n    x = Dropout(config.DROPOUT_RATE, name='dropout2')(x)\n    \n    output = Dense(1, activation='sigmoid', name='output')(x)\n    \n    model = Model(inputs=input_layer, outputs=output, name='DeepfakeDetector_B0_CPU')\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# CPU-FRIENDLY TRAINING\n# ============================================================================\n\ndef train_minimal_model(train_gen, val_gen, val_files, val_labels, config=MinimalConfig):\n    \"\"\"CPU-optimized training\"\"\"\n    \n    print(\"\\nBuilding minimal model...\")\n    model = build_minimal_model(config)\n    \n    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n    total_params = model.count_params()\n    \n    print(f\"Total params: {total_params:,}\")\n    print(f\"Trainable params: {trainable_params:,}\")\n    \n    optimizer = Adam(learning_rate=config.LEARNING_RATE)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss='binary_crossentropy',\n        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n    )\n    \n    callbacks = [\n        SimplePredictionMonitor(val_files, val_labels, max_samples=50),\n        EarlyStopping(\n            monitor='val_auc',\n            patience=config.EARLY_STOP_PATIENCE,\n            restore_best_weights=True,\n            mode='max',\n            verbose=1\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=config.REDUCE_LR_PATIENCE,\n            min_lr=1e-7,\n            verbose=1\n        ),\n        ModelCheckpoint(\n            os.path.join(config.BASE_OUTPUT_PATH, 'best_model_cpu.keras'),\n            monitor='val_auc',\n            save_best_only=True,\n            mode='max',\n            verbose=1\n        )\n    ]\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"TRAINING ON CPU\")\n    print(f\"  Batch Size: {config.BATCH_SIZE}\")\n    print(f\"  Epochs: {config.EPOCHS}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=config.EPOCHS,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Free memory\n    gc.collect()\n    \n    return model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# SIMPLE DATA LOADING\n# ============================================================================\n\ndef load_preprocessed_data_simple(preproc_dir):\n    \"\"\"Load preprocessed data without verification\"\"\"\n    all_face_files = sorted(glob.glob(os.path.join(preproc_dir, \"*_face.jpg\")))\n    \n    if len(all_face_files) == 0:\n        print(\"ERROR: No face files found!\")\n        return None, None, None\n    \n    file_paths = []\n    labels = []\n    video_groups = []\n    \n    for face_path in all_face_files:\n        basename = os.path.basename(face_path)\n        parts = basename.replace('_face.jpg', '').split('_')\n        \n        if len(parts) < 3:\n            continue\n        \n        label_str = parts[0]\n        video_id_str = parts[1]\n        \n        if label_str not in ['real', 'fake']:\n            continue\n        \n        label = 0 if label_str == 'real' else 1\n        video_id = f\"{label_str}_{video_id_str}\"\n        \n        file_paths.append(face_path)\n        labels.append(label)\n        video_groups.append(video_id)\n    \n    print(f\"\\nLoaded {len(file_paths)} samples\")\n    print(f\"  Real: {labels.count(0)}, Fake: {labels.count(1)}\")\n    \n    return file_paths, np.array(labels), np.array(video_groups)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ============================================================================\n# SIMPLE FILENAME-BASED SPLIT\n# ============================================================================\n\ndef simple_split_by_video(file_paths, labels, video_groups, test_size=0.2, val_size=0.2):\n    \"\"\"Simple deterministic split based on video IDs\"\"\"\n    \n    unique_videos = sorted(set(video_groups))\n    np.random.seed(42)\n    np.random.shuffle(unique_videos)\n    \n    n_videos = len(unique_videos)\n    n_test = int(n_videos * test_size)\n    n_val = int(n_videos * val_size)\n    \n    test_videos = set(unique_videos[:n_test])\n    val_videos = set(unique_videos[n_test:n_test+n_val])\n    train_videos = set(unique_videos[n_test+n_val:])\n    \n    train_paths, train_labels = [], []\n    val_paths, val_labels = [], []\n    test_paths, test_labels = [], []\n    \n    for i, video_id in enumerate(video_groups):\n        if video_id in train_videos:\n            train_paths.append(file_paths[i])\n            train_labels.append(labels[i])\n        elif video_id in val_videos:\n            val_paths.append(file_paths[i])\n            val_labels.append(labels[i])\n        elif video_id in test_videos:\n            test_paths.append(file_paths[i])\n            test_labels.append(labels[i])\n    \n    print(f\"\\nSplit complete:\")\n    print(f\"  Train: {len(train_paths)} samples\")\n    print(f\"  Val:   {len(val_paths)} samples\")\n    print(f\"  Test:  {len(test_paths)} samples\")\n    \n    return (train_paths, np.array(train_labels)), (val_paths, np.array(val_labels)), (test_paths, np.array(test_labels))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SIMPLE EVALUATION (NO TTA)\n# ============================================================================\n\ndef evaluate_simple(model, test_gen, config=MinimalConfig):\n    \"\"\"Simple evaluation without TTA\"\"\"\n    \n    print(\"\\nEvaluating model...\")\n    \n    y_pred_list = []\n    y_true_list = []\n    \n    for i in tqdm(range(len(test_gen)), desc=\"Evaluating\"):\n        X_batch, y_batch = test_gen[i]\n        pred_batch = model.predict(X_batch, verbose=0)\n        \n        y_pred_list.extend(pred_batch.flatten())\n        y_true_list.extend(y_batch)\n    \n    y_pred_proba = np.array(y_pred_list)\n    y_true = np.array(y_true_list)\n    \n    # Find best threshold\n    best_threshold = 0.5\n    best_acc = 0\n    \n    for thresh in np.linspace(0.3, 0.7, 9):\n        y_pred_thresh = (y_pred_proba > thresh).astype(int)\n        acc = accuracy_score(y_true, y_pred_thresh)\n        if acc > best_acc:\n            best_acc = acc\n            best_threshold = thresh\n    \n    y_pred = (y_pred_proba > best_threshold).astype(int)\n    \n    accuracy = accuracy_score(y_true, y_pred)\n    auc_score = roc_auc_score(y_true, y_pred_proba)\n    cm = confusion_matrix(y_true, y_pred)\n    report = classification_report(y_true, y_pred, target_names=['Real', 'Fake'], output_dict=True)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EVALUATION RESULTS\")\n    print(\"=\"*80)\n    print(f\"Accuracy:  {accuracy:.4f}\")\n    print(f\"AUC:       {auc_score:.4f}\")\n    print(f\"Threshold: {best_threshold:.2f}\")\n    print(f\"\\nConfusion Matrix:\")\n    print(f\"  TN: {cm[0,0]:4d}  FP: {cm[0,1]:4d}\")\n    print(f\"  FN: {cm[1,0]:4d}  TP: {cm[1,1]:4d}\")\n    print(f\"\\nReal - Precision: {report['Real']['precision']:.4f}, Recall: {report['Real']['recall']:.4f}\")\n    print(f\"Fake - Precision: {report['Fake']['precision']:.4f}, Recall: {report['Fake']['recall']:.4f}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Simple confusion matrix plot\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n    plt.title(f'Confusion Matrix\\nAcc: {accuracy:.4f} | AUC: {auc_score:.4f}')\n    plt.savefig(os.path.join(config.BASE_OUTPUT_PATH, 'confusion_matrix.png'), dpi=100)\n    plt.close()\n    \n    results = {\n        'accuracy': float(accuracy),\n        'auc': float(auc_score),\n        'best_threshold': float(best_threshold),\n        'confusion_matrix': cm.tolist(),\n        'classification_report': report\n    }\n    \n    return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MAIN PIPELINE\n# ============================================================================\n\ndef run_minimal_pipeline():\n    \"\"\"Complete CPU-friendly pipeline\"\"\"\n    \n    print(\"=\"*80)\n    print(\"CPU-OPTIMIZED DEEPFAKE DETECTION PIPELINE\")\n    print(\"=\"*80)\n    \n    config = MinimalConfig()\n    \n    preproc_dir = os.path.join(config.BASE_OUTPUT_PATH, \"minimal_preprocessed\")\n    \n    # ========================================================================\n    # STEP 1: PREPROCESSING\n    # ========================================================================\n    \n    if not os.path.exists(preproc_dir):\n        print(\"\\n\" + \"=\"*80)\n        print(\"STEP 1: PREPROCESSING\")\n        print(\"=\"*80)\n        \n        os.makedirs(preproc_dir, exist_ok=True)\n        \n        extractor = MinimalFaceExtractor(config)\n        \n        real_dir = os.path.join(config.BASE_INPUT_PATH, \"DFD_original sequences\")\n        fake_dir = os.path.join(\n            config.BASE_INPUT_PATH, \n            \"DFD_manipulated_sequences\", \n            \"DFD_manipulated_sequences\"\n        )\n        \n        real_videos = sorted([os.path.join(real_dir, f)\n                              for f in os.listdir(real_dir) if f.endswith(\".mp4\")])\n        \n        fake_videos = sorted([os.path.join(fake_dir, f)\n                              for f in os.listdir(fake_dir) if f.endswith(\".mp4\")])\n        \n        # Balance datasets\n        min_count = min(len(real_videos), len(fake_videos))\n        if config.MAX_VIDEOS_TO_PROCESS:\n            min_count = min(min_count, config.MAX_VIDEOS_TO_PROCESS)\n        \n        real_videos = real_videos[:min_count]\n        fake_videos = fake_videos[:min_count]\n        \n        print(f\"Processing {min_count} real and {min_count} fake videos\")\n        print(f\"Extracting {config.FRAMES_PER_VIDEO} frames per video\\n\")\n        \n        print(\"Processing real videos...\")\n        for video_id, video_path in enumerate(tqdm(real_videos, desc=\"Real\")):\n            extractor.process_video(video_path, preproc_dir, 'real', video_id)\n            \n            # Free memory every 10 videos\n            if video_id % 10 == 0:\n                gc.collect()\n        \n        print(\"Processing fake videos...\")\n        for video_id, video_path in enumerate(tqdm(fake_videos, desc=\"Fake\")):\n            extractor.process_video(video_path, preproc_dir, 'fake', video_id)\n            \n            if video_id % 10 == 0:\n                gc.collect()\n        \n        print(\"\\nPreprocessing complete!\")\n        for key, value in sorted(extractor.stats.items()):\n            print(f\"  {key}: {value}\")\n        \n        del extractor\n        gc.collect()\n    else:\n        print(f\"\\n✓ Using existing preprocessed data: {preproc_dir}\")\n    \n    # ========================================================================\n    # STEP 2: LOAD DATA\n    # ========================================================================\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 2: LOADING DATA\")\n    print(\"=\"*80)\n    \n    file_paths, labels, video_groups = load_preprocessed_data_simple(preproc_dir)\n    \n    if file_paths is None or len(file_paths) < 50:\n        print(\"ERROR: Insufficient data!\")\n        return None\n    \n    train_data, val_data, test_data = simple_split_by_video(\n        file_paths, labels, video_groups,\n        test_size=config.TEST_SPLIT,\n        val_size=config.VALIDATION_SPLIT\n    )\n    \n    train_paths, train_labels = train_data\n    val_paths, val_labels = val_data\n    test_paths, test_labels = test_data\n    \n    print(\"\\nCreating minimal generators...\")\n    train_gen = UltraMinimalGenerator(\n        train_paths, train_labels,\n        batch_size=config.BATCH_SIZE,\n        shuffle=True,\n        augment=True\n    )\n    \n    val_gen = UltraMinimalGenerator(\n        val_paths, val_labels,\n        batch_size=config.BATCH_SIZE,\n        shuffle=False,\n        augment=False\n    )\n    \n    test_gen = UltraMinimalGenerator(\n        test_paths, test_labels,\n        batch_size=config.BATCH_SIZE,\n        shuffle=False,\n        augment=False\n    )\n    \n    # ========================================================================\n    # STEP 3: TRAIN\n    # ========================================================================\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 3: TRAINING\")\n    print(\"=\"*80)\n    \n    tf.keras.backend.clear_session()\n    gc.collect()\n    \n    model, history = train_minimal_model(train_gen, val_gen, val_paths, val_labels, config)\n    \n    # Save history\n    history_dict = {k: [float(v) for v in vals] for k, vals in history.history.items()}\n    with open(os.path.join(config.BASE_OUTPUT_PATH, \"history_minimal.json\"), \"w\") as f:\n        json.dump(history_dict, f, indent=4)\n    \n    # ========================================================================\n    # STEP 4: EVALUATE\n    # ========================================================================\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 4: EVALUATION\")\n    print(\"=\"*80)\n    \n    results = evaluate_simple(model, test_gen, config)\n    \n    with open(os.path.join(config.BASE_OUTPUT_PATH, \"results_minimal.json\"), \"w\") as f:\n        json.dump(results, f, indent=4)\n    \n    model.save(os.path.join(config.BASE_OUTPUT_PATH, \"final_model_cpu.keras\"))\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"PIPELINE COMPLETE!\")\n    print(\"=\"*80)\n    \n    gc.collect()\n    \n    return results\n\n# ============================================================================\n# RUN PIPELINE\n# ============================================================================\n\nif __name__ == \"__main__\":\n    print(\"\\nStarting CPU-optimized pipeline...\")\n    print(\"This is designed to run for 12+ hours on Kaggle CPU without crashes.\\n\")\n    \n    results = run_minimal_pipeline()\n    \n    print(\"\\nAll done! Check /kaggle/working for outputs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:12:08.235489Z","iopub.execute_input":"2025-11-25T05:12:08.235747Z","iopub.status.idle":"2025-11-25T06:34:56.901462Z","shell.execute_reply.started":"2025-11-25T05:12:08.235730Z","shell.execute_reply":"2025-11-25T06:34:56.900096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **TESTING**","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\n# ============================================================================\n# Load face detector\n# ============================================================================\n\nPROTOTXT = \"/kaggle/working/deploy.prototxt\"\nCAFFEMODEL = \"/kaggle/working/res10_300x300_ssd_iter_140000.caffemodel\"\n\ndnn_face_net = cv2.dnn.readNetFromCaffe(PROTOTXT, CAFFEMODEL)\n\ndef detect_face(frame):\n    \"\"\"Return the largest detected face or None.\"\"\"\n    h, w = frame.shape[:2]\n    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n                                 (104.0, 177.0, 123.0), swapRB=False)\n    dnn_face_net.setInput(blob)\n    detections = dnn_face_net.forward()\n\n    best = None\n    best_area = 0\n\n    for i in range(detections.shape[2]):\n        conf = detections[0, 0, i, 2]\n        if conf < 0.3:\n            continue\n\n        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n        x1, y1, x2, y2 = box.astype(int)\n        area = (x2 - x1) * (y2 - y1)\n\n        if area > best_area:\n            best = frame[y1:y2, x1:x2]\n            best_area = area\n\n    return best\n\n\n# ============================================================================\n# Predict on video (simple)\n# ============================================================================\n\ndef test_video(model, video_path, num_frames=15):\n    cap = cv2.VideoCapture(video_path)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    indices = np.linspace(0, total - 1, num_frames).astype(int)\n\n    preds = []\n\n    for idx in indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if not ret:\n            continue\n\n        face = detect_face(frame)\n        if face is None:\n            continue\n\n        face = cv2.resize(face, (224, 224))\n        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n        face = preprocess_input(np.expand_dims(face.astype(\"float32\"), 0))\n        pred = float(model.predict(face, verbose=0)[0][0])\n        preds.append(pred)\n\n    cap.release()\n\n    if len(preds) == 0:\n        print(f\"{video_path}: No face detected\")\n        return None, None\n\n    preds = np.array(preds)\n\n    # MAX score strategy\n    max_score = preds.max()\n    threshold = 0.35\n    label = \"FAKE\" if max_score > threshold else \"REAL\"\n\n    print(f\"{os.path.basename(video_path)} → {label} (score={max_score:.4f})\")\n    return label, max_score\n\n\n# ============================================================================\n# Batch testing (simple)\n# ============================================================================\n\ndef test_videos(model, video_list):\n    for v in video_list:\n        test_video(model, v)\n\n\n# ============================================================================\n# Usage\n# ============================================================================\n\nif __name__ == \"__main__\":\n    model = tf.keras.models.load_model(\"/kaggle/working/final_model_cpu.keras\")\n\n    videos = [\n        \"/kaggle/input/testing/real3.mp4\",\n        \"/kaggle/input/testing/fake1.mp4\",\n        \"/kaggle/input/testing/fake2.mp4\",\n        \"/kaggle/input/testing/fake3.mp4\",\n        \"/kaggle/input/testing/real2.mp4\",\n        \"/kaggle/input/testing/real.mp4\",\n        \"/kaggle/input/testing/Deepfake video of Volodymyr Zelensky surrendering surfaces on social media (1).mp4\",\n        \"/kaggle/input/testing/Deepfake Example Presented by Senator Richard Blumenthal.mp4\",\n        \"/kaggle/input/testing/MrBeast and BBC stars used in deepfake scam videos - BBC News.mp4\"\n    ]\n\n    test_videos(model, videos)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:33:09.916032Z","iopub.execute_input":"2025-11-25T09:33:09.916373Z","iopub.status.idle":"2025-11-25T09:33:47.386685Z","shell.execute_reply.started":"2025-11-25T09:33:09.916350Z","shell.execute_reply":"2025-11-25T09:33:47.385346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Root Cause: Your model was trained on DFD dataset (studio-quality), \n# but real videos are from YouTube/TikTok (compression-heavy).\n\n# DFD real videos =\n# high resolution\n# perfect lighting\n# DSLR-quality\n# no compression artifacts\n\n# Our real-world videos =\n# overcompressed\n# color-shifted\n# blurry\n# different lighting\n# different distribution entirely\n\n# ➡ Our model has learned “compression artifacts = FAKE”\n# Because in DFD:\n# REAL videos are clean\n# FAKE videos are compressed or manipulated\n\n# So dataset biasness is there.\n\n# real2.mp4 → FAKE (0.8260)\n# real.mp4 → FAKE (0.9996)\n# This is NOT a face-detection problem\n# This is NOT a testing code problem either.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:10:44.725715Z","iopub.execute_input":"2025-11-25T09:10:44.726972Z","iopub.status.idle":"2025-11-25T09:10:44.732015Z","shell.execute_reply.started":"2025-11-25T09:10:44.726927Z","shell.execute_reply":"2025-11-25T09:10:44.730701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Final Metrics and Visualization** ","metadata":{}},{"cell_type":"code","source":"import glob\nimport numpy as np\n\npreproc_dir = \"/kaggle/working/minimal_preprocessed\"\n\nfile_paths = sorted(glob.glob(preproc_dir + \"/*_face.jpg\"))\nlabels = np.array([0 if \"real_\" in f else 1 for f in file_paths])\n\nprint(\"Recovered:\", len(file_paths), \"files\")\nprint(\"Real:\", (labels == 0).sum(), \"Fake:\", (labels == 1).sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:10:52.062645Z","iopub.execute_input":"2025-11-25T09:10:52.062917Z","iopub.status.idle":"2025-11-25T09:10:52.080724Z","shell.execute_reply.started":"2025-11-25T09:10:52.062902Z","shell.execute_reply":"2025-11-25T09:10:52.079645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom matplotlib import pyplot as plt\n\nreal_samples = [f for f in file_paths if \"/real_\" in f]\nfake_samples = [f for f in file_paths if \"/fake_\" in f]\n\nreal_sample = random.choice(real_samples)\nfake_sample = random.choice(fake_samples)\n\ndef show_image(img_path, title):\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.title(title)\n    plt.axis(\"off\")\n\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nshow_image(real_sample, \"Sample Real Face\")\n\nplt.subplot(1,2,2)\nshow_image(fake_sample, \"Sample Fake Face\")\nplt.savefig(\"/kaggle/working/sample_real_fake.png\", dpi=120)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:44:59.286679Z","iopub.execute_input":"2025-11-23T09:44:59.287188Z","iopub.status.idle":"2025-11-23T09:44:59.873912Z","shell.execute_reply.started":"2025-11-23T09:44:59.287156Z","shell.execute_reply":"2025-11-23T09:44:59.872539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\n\nbase_input = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset\"\n\nreal_dir = os.path.join(base_input, \"DFD_original sequences\")\nfake_dir = os.path.join(base_input, \"DFD_manipulated_sequences\", \"DFD_manipulated_sequences\")\n\nreal_videos = sorted(glob.glob(real_dir + \"/*.mp4\"))\nfake_videos = sorted(glob.glob(fake_dir + \"/*.mp4\"))\n\nprint(\"Real videos:\", len(real_videos))\nprint(\"Fake videos:\", len(fake_videos))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:48:34.333015Z","iopub.execute_input":"2025-11-23T09:48:34.333482Z","iopub.status.idle":"2025-11-23T09:48:34.419624Z","shell.execute_reply.started":"2025-11-23T09:48:34.333451Z","shell.execute_reply":"2025-11-23T09:48:34.418420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/working/history_minimal.json\") as f:\n    history_data = json.load(f)\n\nprint(history_data.keys())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:52:58.522467Z","iopub.execute_input":"2025-11-23T09:52:58.524150Z","iopub.status.idle":"2025-11-23T09:52:58.531146Z","shell.execute_reply.started":"2025-11-23T09:52:58.524099Z","shell.execute_reply":"2025-11-23T09:52:58.530012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/working/results_minimal.json\") as f:\n    results = json.load(f)\n\ny_true = np.array(results[\"y_true\"]) if \"y_true\" in results else None\ny_pred_proba = np.array(results[\"y_pred_proba\"]) if \"y_pred_proba\" in results else None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:54:09.315366Z","iopub.execute_input":"2025-11-23T09:54:09.315730Z","iopub.status.idle":"2025-11-23T09:54:09.323551Z","shell.execute_reply.started":"2025-11-23T09:54:09.315706Z","shell.execute_reply":"2025-11-23T09:54:09.322372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport numpy as np\nimport os\n\npreproc_dir = \"/kaggle/working/minimal_preprocessed\"\n\nall_face_files = sorted(glob.glob(preproc_dir + \"/*_face.jpg\"))\n\nfile_paths = []\nlabels = []\nvideo_groups = []\n\nfor face_path in all_face_files:\n    basename = os.path.basename(face_path)\n    parts = basename.replace(\"_face.jpg\", \"\").split(\"_\")\n\n    label = 0 if \"real\" in parts[0] else 1\n    video_id = f\"{parts[0]}_{parts[1]}\"\n\n    file_paths.append(face_path)\n    labels.append(label)\n    video_groups.append(video_id)\n\nfile_paths = np.array(file_paths)\nlabels = np.array(labels)\nvideo_groups = np.array(video_groups)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:56:16.368639Z","iopub.execute_input":"2025-11-23T09:56:16.369207Z","iopub.status.idle":"2025-11-23T09:56:16.403765Z","shell.execute_reply.started":"2025-11-23T09:56:16.369178Z","shell.execute_reply":"2025-11-23T09:56:16.402451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def simple_split_by_video(file_paths, labels, video_groups, test_size=0.2, val_size=0.2):\n    unique_videos = sorted(set(video_groups))\n    np.random.seed(42)\n    np.random.shuffle(unique_videos)\n\n    n = len(unique_videos)\n    n_test = int(n * test_size)\n    n_val = int(n * val_size)\n\n    test_videos = set(unique_videos[:n_test])\n    val_videos = set(unique_videos[n_test:n_test+n_val])\n    train_videos = set(unique_videos[n_test+n_val:])\n\n    train_paths, train_labels = [], []\n    val_paths, val_labels = [], []\n    test_paths, test_labels = [], []\n\n    for i, vid in enumerate(video_groups):\n        if vid in train_videos:\n            train_paths.append(file_paths[i])\n            train_labels.append(labels[i])\n        elif vid in val_videos:\n            val_paths.append(file_paths[i])\n            val_labels.append(labels[i])\n        elif vid in test_videos:\n            test_paths.append(file_paths[i])\n            test_labels.append(labels[i])\n\n    return (train_paths, np.array(train_labels)), (val_paths, np.array(val_labels)), (test_paths, np.array(test_labels))\n\n\n(train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = simple_split_by_video(\n    file_paths, labels, video_groups, test_size=0.2, val_size=0.2\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:56:25.654759Z","iopub.execute_input":"2025-11-23T09:56:25.656074Z","iopub.status.idle":"2025-11-23T09:56:25.682168Z","shell.execute_reply.started":"2025-11-23T09:56:25.656010Z","shell.execute_reply":"2025-11-23T09:56:25.681017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"/kaggle/working/final_model_cpu.keras\")\n\ntest_gen = UltraMinimalGenerator(test_paths, test_labels, batch_size=8, shuffle=False)\n\ny_pred_proba = []\ny_true = []\n\nfor i in range(len(test_gen)):\n    X, y = test_gen[i]\n    pred = model.predict(X, verbose=0).flatten()\n    y_pred_proba.extend(pred)\n    y_true.extend(y)\n\ny_pred_proba = np.array(y_pred_proba)\ny_true = np.array(y_true)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:56:34.560898Z","iopub.execute_input":"2025-11-23T09:56:34.562253Z","iopub.status.idle":"2025-11-23T09:57:28.195880Z","shell.execute_reply.started":"2025-11-23T09:56:34.562178Z","shell.execute_reply":"2025-11-23T09:57:28.194828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\n# Load saved training history\nwith open(\"/kaggle/working/history_minimal.json\") as f:\n    history_data = json.load(f)\n\nplt.figure(figsize=(12,5))\n\n# Loss Curve\nplt.subplot(1,2,1)\nplt.plot(history_data[\"loss\"], label=\"Train Loss\")\nplt.plot(history_data[\"val_loss\"], label=\"Val Loss\")\nplt.legend()\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\n\n# Accuracy Curve\nplt.subplot(1,2,2)\nplt.plot(history_data[\"accuracy\"], label=\"Train Accuracy\")\nplt.plot(history_data[\"val_accuracy\"], label=\"Val Accuracy\")\nplt.legend()\nplt.title(\"Accuracy Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\n\nplt.savefig(\"/kaggle/working/training_curves.png\", dpi=120)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:58:34.606071Z","iopub.execute_input":"2025-11-23T09:58:34.606486Z","iopub.status.idle":"2025-11-23T09:58:35.197776Z","shell.execute_reply.started":"2025-11-23T09:58:34.606460Z","shell.execute_reply":"2025-11-23T09:58:35.196579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nfpr, tpr, _ = roc_curve(y_true, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.4f}\")\nplt.plot([0,1], [0,1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\n\nplt.savefig(\"/kaggle/working/roc_curve.png\", dpi=120)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:58:44.282327Z","iopub.execute_input":"2025-11-23T09:58:44.282662Z","iopub.status.idle":"2025-11-23T09:58:44.600258Z","shell.execute_reply.started":"2025-11-23T09:58:44.282641Z","shell.execute_reply":"2025-11-23T09:58:44.598920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score\n\nthresholds = np.linspace(0.3, 0.7, 50)\nacc_list = []\n\nfor t in thresholds:\n    preds = (y_pred_proba > t).astype(int)\n    acc_list.append(accuracy_score(y_true, preds))\n\nplt.figure(figsize=(6,4))\nplt.plot(thresholds, acc_list)\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Threshold vs Accuracy\")\nplt.grid()\n\nplt.savefig(\"/kaggle/working/threshold_vs_accuracy.png\", dpi=120)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T10:04:47.358174Z","iopub.execute_input":"2025-11-23T10:04:47.358562Z","iopub.status.idle":"2025-11-23T10:04:47.666985Z","shell.execute_reply.started":"2025-11-23T10:04:47.358538Z","shell.execute_reply":"2025-11-23T10:04:47.665980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Compute best threshold (same logic as your pipeline)\nthresholds = np.linspace(0.3, 0.7, 50)\nbest_acc = 0\nbest_threshold = 0.5\n\nfor t in thresholds:\n    preds = (y_pred_proba > t).astype(int)\n    acc = accuracy_score(y_true, preds)\n    if acc > best_acc:\n        best_acc = acc\n        best_threshold = t\n\nprint(\"Best threshold:\", best_threshold)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T10:00:24.710570Z","iopub.execute_input":"2025-11-23T10:00:24.710983Z","iopub.status.idle":"2025-11-23T10:00:24.745289Z","shell.execute_reply.started":"2025-11-23T10:00:24.710961Z","shell.execute_reply":"2025-11-23T10:00:24.743751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = (y_pred_proba > best_threshold).astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T10:00:31.785763Z","iopub.execute_input":"2025-11-23T10:00:31.786105Z","iopub.status.idle":"2025-11-23T10:00:31.792552Z","shell.execute_reply.started":"2025-11-23T10:00:31.786086Z","shell.execute_reply":"2025-11-23T10:00:31.791080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Real', 'Fake'],\n            yticklabels=['Real', 'Fake'])\n\nplt.title(f\"Confusion Matrix\\nAccuracy={best_acc:.4f}, Threshold={best_threshold:.2f}\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.savefig(\"/kaggle/working/confusion_matrix.png\", dpi=120)\nplt.show()\n\nprint(\"TN, FP, FN, TP:\", cm.ravel())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T10:04:04.944325Z","iopub.execute_input":"2025-11-23T10:04:04.944774Z","iopub.status.idle":"2025-11-23T10:04:05.195421Z","shell.execute_reply.started":"2025-11-23T10:04:04.944717Z","shell.execute_reply":"2025-11-23T10:04:05.194423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/full_dataset_preprocessed\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}